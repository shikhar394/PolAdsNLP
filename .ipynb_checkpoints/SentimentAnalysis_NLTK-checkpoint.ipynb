{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "#!pip install unidecode\n",
    "#!pip install pprint\n",
    "#!pip install pandas\n",
    "from pprint import pprint\n",
    "import json\n",
    "from unidecode import unidecode\n",
    "import html\n",
    "import re\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import subjectivity\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.sentiment.util import *\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['smart', 'and', 'alert', ',', 'thirteen', 'conversations', 'about', 'one', 'thing', 'is', 'a', 'small', 'gem', '.'], 'subj'), (['color', ',', 'musical', 'bounce', 'and', 'warm', 'seas', 'lapping', 'on', 'island', 'shores', '.', 'and', 'just', 'enough', 'science', 'to', 'send', 'you', 'home', 'thinking', '.'], 'subj')]\n",
      "1000 1000\n"
     ]
    }
   ],
   "source": [
    "n_instances = 1000\n",
    "subj_docs = [(sent, 'subj') for sent in subjectivity.sents(categories='subj')[:n_instances]]\n",
    "obj_docs = [(sent, 'obj') for sent in subjectivity.sents(categories='obj')[:n_instances]]\n",
    "print(subj_docs[:2])\n",
    "print(len(subj_docs), len(obj_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subj_docs = subj_docs[:800]\n",
    "test_subj_docs = subj_docs[800:1000]\n",
    "train_obj_docs = obj_docs[:800]\n",
    "test_obj_docs = obj_docs[800:1000]\n",
    "training_docs = train_subj_docs+train_obj_docs\n",
    "testing_docs = test_subj_docs+test_obj_docs\n",
    "sentim_analyzer = SentimentAnalyzer()\n",
    "all_words_neg = sentim_analyzer.all_words([mark_negation(doc) for doc in training_docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_feats = sentim_analyzer.unigram_word_feats(all_words_neg, min_freq=4)\n",
    "sentim_analyzer.add_feat_extractor(extract_unigram_feats, unigrams=unigram_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = sentim_analyzer.apply_features(training_docs)\n",
    "test_set = sentim_analyzer.apply_features(testing_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classifier\n",
      "Evaluating NaiveBayesClassifier results...\n",
      "Accuracy: 0.835\n",
      "F-measure [obj]: 0.8263157894736842\n",
      "F-measure [subj]: 0.8428571428571429\n",
      "Precision [obj]: 0.8722222222222222\n",
      "Precision [subj]: 0.8045454545454546\n",
      "Recall [obj]: 0.785\n",
      "Recall [subj]: 0.885\n"
     ]
    }
   ],
   "source": [
    "trainer = NaiveBayesClassifier.train\n",
    "classifier = sentim_analyzer.train(trainer, training_set)\n",
    "for key,value in sorted(sentim_analyzer.evaluate(test_set).items()):\n",
    "    print('{0}: {1}'.format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "TagRemove = re.compile(r'<[^>]+>')\n",
    "\n",
    "\n",
    "def RemoveTags(text):\n",
    "    return TagRemove.sub(\"\", text)\n",
    "\n",
    "def CleanText(text):\n",
    "    return unidecode(RemoveTags(html.unescape(text.strip())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "textDict = {}\n",
    "with open('TextSample.json') as f, open(\"TextSampleLabel.json\", 'w') as f1:\n",
    "    JSONData = json.load(f)\n",
    "    writer = csv.writer(f1)\n",
    "    for (ID, text) in JSONData.items():\n",
    "        text = CleanText(text)\n",
    "        textDict[ID] = [text, sid.polarity_scores(text)]\n",
    "        json.dump(textDict, f1)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NEWcrawl_201873112Contents',\n",
       " 'Crawl_config.cfg',\n",
       " 'TextSampleLabel.csv',\n",
       " 'FBAdScrapeScriptNew.py',\n",
       " 'GetAdSnapshotsCCS1.py',\n",
       " '.DS_Store',\n",
       " 'LICENSE',\n",
       " 'requirements.txt',\n",
       " 'NEWcrawl_201873113Contents',\n",
       " 'crawl_201872312',\n",
       " 'RawContentFiles',\n",
       " 'NEWcrawl_201872311',\n",
       " 'FBAdScrapeScript.py',\n",
       " 'TextSample.json',\n",
       " 'TestSeeds.txt',\n",
       " 'NEWcrawl_201873111Contents',\n",
       " 'Downloading_ads_06_05_v1.py',\n",
       " 'getSampleTextfromDB.py',\n",
       " 'TextSampleLabel.json',\n",
       " 'NEWcrawl_201873110Contents',\n",
       " 'docs',\n",
       " 'Instructions.txt',\n",
       " 'MasterSeeds.txt',\n",
       " 'import_ads_to_db.py',\n",
       " 'NEWcrawl_201872216',\n",
       " 'geckodriver.log',\n",
       " '.gitignore',\n",
       " '_site',\n",
       " 'NEWcrawl_20188111Contents',\n",
       " 'NEWcrawl_201873114Contents',\n",
       " 'Gemfile',\n",
       " 'Gemfile.lock',\n",
       " 'crawl_201872311',\n",
       " 'Test.txt',\n",
       " '.ipynb_checkpoints',\n",
       " 'SentimentAnalysis_NLTK.ipynb',\n",
       " 'GetAdSnapshotsCCS2.py',\n",
       " 'NEWcrawl_201872312',\n",
       " 'NEWcrawl_201873120Contents',\n",
       " '.git',\n",
       " 'fbpolads.sql',\n",
       " 'NEWcrawl_20188112Contents',\n",
       " 'CheckNewAds.py']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
