{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /anaconda3/lib/python3.6/site-packages (0.23.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /anaconda3/lib/python3.6/site-packages (from pandas) (2.7.3)\n",
      "Requirement already satisfied: pytz>=2011k in /anaconda3/lib/python3.6/site-packages (from pandas) (2018.4)\n",
      "Requirement already satisfied: numpy>=1.9.0 in /anaconda3/lib/python3.6/site-packages (from pandas) (1.14.3)\n",
      "Requirement already satisfied: six>=1.5 in /anaconda3/lib/python3.6/site-packages (from python-dateutil>=2.5.0->pandas) (1.11.0)\n",
      "\u001b[31mdistributed 1.21.8 requires msgpack, which is not installed.\u001b[0m\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 18.0 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: numpy in /anaconda3/lib/python3.6/site-packages (1.14.3)\n",
      "\u001b[31mdistributed 1.21.8 requires msgpack, which is not installed.\u001b[0m\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 18.0 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting re\n",
      "\u001b[31m  Could not find a version that satisfies the requirement re (from versions: )\u001b[0m\n",
      "\u001b[31mNo matching distribution found for re\u001b[0m\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 18.0 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: gensim in /anaconda3/lib/python3.6/site-packages (3.6.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /anaconda3/lib/python3.6/site-packages (from gensim) (1.1.0)\n",
      "Requirement already satisfied: smart-open>=1.2.1 in /anaconda3/lib/python3.6/site-packages (from gensim) (1.7.1)\n",
      "Requirement already satisfied: six>=1.5.0 in /anaconda3/lib/python3.6/site-packages (from gensim) (1.11.0)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /anaconda3/lib/python3.6/site-packages (from gensim) (1.14.3)\n",
      "Requirement already satisfied: bz2file in /anaconda3/lib/python3.6/site-packages (from smart-open>=1.2.1->gensim) (0.98)\n",
      "Requirement already satisfied: boto>=2.32 in /anaconda3/lib/python3.6/site-packages (from smart-open>=1.2.1->gensim) (2.48.0)\n",
      "Requirement already satisfied: boto3 in /anaconda3/lib/python3.6/site-packages (from smart-open>=1.2.1->gensim) (1.9.14)\n",
      "Requirement already satisfied: requests in /anaconda3/lib/python3.6/site-packages (from smart-open>=1.2.1->gensim) (2.18.4)\n",
      "Requirement already satisfied: botocore<1.13.0,>=1.12.14 in /anaconda3/lib/python3.6/site-packages (from boto3->smart-open>=1.2.1->gensim) (1.12.14)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /anaconda3/lib/python3.6/site-packages (from boto3->smart-open>=1.2.1->gensim) (0.9.3)\n",
      "Requirement already satisfied: s3transfer<0.2.0,>=0.1.10 in /anaconda3/lib/python3.6/site-packages (from boto3->smart-open>=1.2.1->gensim) (0.1.13)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /anaconda3/lib/python3.6/site-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /anaconda3/lib/python3.6/site-packages (from requests->smart-open>=1.2.1->gensim) (2.6)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /anaconda3/lib/python3.6/site-packages (from requests->smart-open>=1.2.1->gensim) (1.22)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /anaconda3/lib/python3.6/site-packages (from requests->smart-open>=1.2.1->gensim) (2018.4.16)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /anaconda3/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.14->boto3->smart-open>=1.2.1->gensim) (2.7.3)\n",
      "Requirement already satisfied: docutils>=0.10 in /anaconda3/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.14->boto3->smart-open>=1.2.1->gensim) (0.14)\n",
      "\u001b[31mdistributed 1.21.8 requires msgpack, which is not installed.\u001b[0m\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 18.0 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: nltk in /anaconda3/lib/python3.6/site-packages (3.3)\n",
      "Requirement already satisfied: six in /anaconda3/lib/python3.6/site-packages (from nltk) (1.11.0)\n",
      "\u001b[31mdistributed 1.21.8 requires msgpack, which is not installed.\u001b[0m\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 18.0 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: scikit-learn in /anaconda3/lib/python3.6/site-packages (0.19.1)\n",
      "\u001b[31mdistributed 1.21.8 requires msgpack, which is not installed.\u001b[0m\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 18.0 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: cython in /anaconda3/lib/python3.6/site-packages (0.28.2)\n",
      "\u001b[31mdistributed 1.21.8 requires msgpack, which is not installed.\u001b[0m\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 18.0 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting tensorflow\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/70/78/cd74769027b6249e45807637c1aa3ef212b9492349cca4b87e5de1a10548/tensorflow-1.11.0-cp36-cp36m-macosx_10_11_x86_64.whl (59.3MB)\n",
      "\u001b[K    100% |████████████████████████████████| 59.4MB 372kB/s ta 0:00:01    71% |██████████████████████▉         | 42.3MB 739kB/s eta 0:00:24\n",
      "\u001b[?25hCollecting grpcio>=1.8.6 (from tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/24/1a/84396834b04cd43be3c10f5faeadf62d01689b704b3c99d09e1e08a41d9b/grpcio-1.15.0-cp36-cp36m-macosx_10_7_intel.whl (2.0MB)\n",
      "\u001b[K    100% |████████████████████████████████| 2.0MB 894kB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /anaconda3/lib/python3.6/site-packages (from tensorflow) (0.31.1)\n",
      "Collecting gast>=0.2.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/5c/78/ff794fcae2ce8aa6323e789d1f8b3b7765f601e7702726f430e814822b96/gast-0.2.0.tar.gz\n",
      "Collecting keras-preprocessing>=1.0.3 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/fc/94/74e0fa783d3fc07e41715973435dd051ca89c550881b3454233c39c73e69/Keras_Preprocessing-1.0.5-py2.py3-none-any.whl\n",
      "Collecting keras-applications>=1.0.5 (from tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/c4/2ff40221029f7098d58f8d7fb99b97e8100f3293f9856f0fb5834bef100b/Keras_Applications-1.0.6-py2.py3-none-any.whl (44kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 1.4MB/s ta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: setuptools<=39.1.0 in /anaconda3/lib/python3.6/site-packages (from tensorflow) (39.1.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /anaconda3/lib/python3.6/site-packages (from tensorflow) (1.14.3)\n",
      "Collecting protobuf>=3.6.0 (from tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c7/27/133f225035b9539f2dcfebcdf9a69ff0152f56e0120160ec5c972ea7deb9/protobuf-3.6.1-cp36-cp36m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (1.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.2MB 1.2MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz\n",
      "Collecting absl-py>=0.1.6 (from tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/16/db/cce5331638138c178dd1d5fb69f3f55eb3787a12efd9177177ae203e847f/absl-py-0.5.0.tar.gz (90kB)\n",
      "\u001b[K    100% |████████████████████████████████| 92kB 909kB/s ta 0:00:011\n",
      "\u001b[?25hCollecting tensorboard<1.12.0,>=1.11.0 (from tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/2f/4d788919b1feef04624d63ed6ea45a49d1d1c834199ec50716edb5d310f4/tensorboard-1.11.0-py3-none-any.whl (3.0MB)\n",
      "\u001b[K    100% |████████████████████████████████| 3.0MB 574kB/s ta 0:00:01\n",
      "\u001b[?25hCollecting astor>=0.6.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/35/6b/11530768cac581a12952a2aad00e1526b89d242d0b9f59534ef6e6a1752f/astor-0.7.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: six>=1.10.0 in /anaconda3/lib/python3.6/site-packages (from tensorflow) (1.11.0)\n",
      "Requirement already satisfied: h5py in /anaconda3/lib/python3.6/site-packages (from keras-applications>=1.0.5->tensorflow) (2.7.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.10 in /anaconda3/lib/python3.6/site-packages (from tensorboard<1.12.0,>=1.11.0->tensorflow) (0.14.1)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<1.12.0,>=1.11.0->tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7a/6b/5600647404ba15545ec37d2f7f58844d690baf2f81f3a60b862e48f29287/Markdown-3.0.1-py2.py3-none-any.whl (89kB)\n",
      "\u001b[K    100% |████████████████████████████████| 92kB 518kB/s ta 0:00:011\n",
      "\u001b[?25hBuilding wheels for collected packages: gast, termcolor, absl-py\n",
      "  Running setup.py bdist_wheel for gast ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/shikharsakhuja/Library/Caches/pip/wheels/9a/1f/0e/3cde98113222b853e98fc0a8e9924480a3e25f1b4008cedb4f\n",
      "  Running setup.py bdist_wheel for termcolor ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/shikharsakhuja/Library/Caches/pip/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6\n",
      "  Running setup.py bdist_wheel for absl-py ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/shikharsakhuja/Library/Caches/pip/wheels/3c/33/ae/db8cd618e62f87594c13a5483f96e618044f9b01596efd013f\n",
      "Successfully built gast termcolor absl-py\n",
      "\u001b[31mdistributed 1.21.8 requires msgpack, which is not installed.\u001b[0m\n",
      "Installing collected packages: grpcio, gast, keras-preprocessing, keras-applications, protobuf, termcolor, absl-py, markdown, tensorboard, astor, tensorflow\n",
      "Successfully installed absl-py-0.5.0 astor-0.7.1 gast-0.2.0 grpcio-1.15.0 keras-applications-1.0.6 keras-preprocessing-1.0.5 markdown-3.0.1 protobuf-3.6.1 tensorboard-1.11.0 tensorflow-1.11.0 termcolor-1.1.0\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 18.0 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install pandas\n",
    "!{sys.executable} -m pip install numpy\n",
    "!{sys.executable} -m pip install re\n",
    "!{sys.executable} -m pip install gensim\n",
    "!{sys.executable} -m pip install nltk\n",
    "!{sys.executable} -m pip install scikit-learn\n",
    "!{sys.executable} -m pip install cython\n",
    "!{sys.executable} -m pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import json\n",
    "import re\n",
    "from gensim import corpora\n",
    "from pprint import pprint\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import random\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "448016\n",
      "215458535964672 ['ve', 'said', 'it', 'before', 'and', 'will', 'say', 'it', 'again', 'the', 'mainstream', 'media', 'is', 'out', 'to', 'bring', 'down', 'my', 'administration', 'need', 'you', 'to', 'take', 'the', 'media', 'accountability', 'survey', 'to', 'do', 'your', 'part', 'to', 'fight', 'back', 'against', 'the', 'fake', 'news', 'attacks', 'and', 'deceptions']\n"
     ]
    }
   ],
   "source": [
    "TEXTFILE = \"AllText.json\"\n",
    "FileJson = json.load(open(TEXTFILE))\n",
    "RawSentences = {}\n",
    "for i, sentenceid in enumerate(FileJson):\n",
    "    RawSentences[int(sentenceid)] = gensim.utils.simple_preprocess(FileJson[sentenceid])\n",
    "    \n",
    "print(len(RawSentences))\n",
    "for k, v in RawSentences.items():\n",
    "    print(k, v)\n",
    "    break\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['ve', 'said', 'it', 'before', 'and', 'will', 'say', 'it', 'again', 'the', 'mainstream', 'media', 'is', 'out', 'to', 'bring', 'down', 'my', 'administration', 'need', 'you', 'to', 'take', 'the', 'media', 'accountability', 'survey', 'to', 'do', 'your', 'part', 'to', 'fight', 'back', 'against', 'the', 'fake', 'news', 'attacks', 'and', 'deceptions'], ['if', 'we', 'want', 'to', 'change', 'congress', 'we', 'need', 'to', 'change', 'who', 'we', 'send', 'to', 'congress'], ['happy', 'th', 'of', 'july', 'let', 'celebrate', 'our', 'freedom', 'and', 'independence', 'today', 'and', 'cherish', 'them', 'always', 'freedom', 'is', 'never', 'more', 'than', 'one', 'generation', 'away', 'from', 'extinction', 'we', 'didn', 'pass', 'it', 'to', 'our', 'children', 'in', 'the', 'bloodstream', 'it', 'must', 'be', 'fought', 'for', 'protected', 'and', 'handed', 'on', 'for', 'them', 'to', 'do', 'the', 'same', 'president', 'reagan'], ['attention', 'nv', 'homeowners', 'the', 'new', 'summer', 'electric', 'rates', 'have', 'started', 'this', 'year', 'your', 'electricity', 'bills', 'will', 'be', 'higher', 'than', 'last', 'year', 'the', 'good', 'news', 'is', 'that', 'nv', 'counties', 'solar', 'program', 'launched', 'no', 'cost', 'solar', 'program', 'year', 'ago', 'and', 'has', 'helped', 'of', 'homeowners', 'go', 'solar', 'if', 'you', 'like', 'to', 'lower', 'your', 'electric', 'bill', 'and', 'enjoy', 'your', 'ac', 'again', 'without', 'fear', 'click', 'the', 'link', 'below', 'to', 'find', 'out', 'if', 'your', 'home', 'qualifies', 'https', 'com', 'no', 'cost', 'solar'], ['made', 'in', 'the', 'usa']]\n",
      "127379\n"
     ]
    }
   ],
   "source": [
    "Sentences = [Sentence for Sentence in RawSentences.values()]\n",
    "print(Sentences[:5])\n",
    "model = Word2Vec(sentences=Sentences, min_count=1, window=5, workers=20, size=300)\n",
    "model.train(Sentences, total_examples=len(Sentences), epochs=20)\n",
    "print(len(model.wv.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"word '__UNSEEN__' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-1aef9911ea67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'__UNSEEN__'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, entities)\u001b[0m\n\u001b[1;32m    335\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0;31m# allow calls like trained_model['office'], as a shorthand for trained_model[['office']]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentity\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentity\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentities\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwords_closer_than\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    450\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"word '__UNSEEN__' not in vocabulary\""
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELSFILE = \"Text.csv\"\n",
    "LabelGroups = [[],[],[],[],[]]\n",
    "def MakeOneHotLabels(label, ID):\n",
    "    \"\"\"\n",
    "    Makes one hot label. 1 at the sentiment. \n",
    "    [1,0,0,0,0] for Very Negative\n",
    "    [0,0,1,0,0] for Neutral\n",
    "    [0,0,0,0,1] for Very Positive\n",
    "    \"\"\"\n",
    "    OneHot = [0,0,0,0,0]\n",
    "    \n",
    "    label = int(label)\n",
    "    OneHot[label+2]+=1 #Initializes at 0.\n",
    "    LabelGroups[label+2].append(ID)\n",
    "    return OneHot\n",
    "    \n",
    "    \n",
    "Labels = {}\n",
    "with open(LABELSFILE, 'r') as f:\n",
    "    File = csv.DictReader(f, delimiter=',')\n",
    "    for row in File:\n",
    "        ID = int(row['ID'])        \n",
    "        if row['ClearMajority'] != '-':\n",
    "            Labels[ID] = MakeOneHotLabels(row['ClearMajority'], ID)\n",
    "        elif row['SoftMajority'] != '-':\n",
    "            Labels[ID] = MakeOneHotLabels(row['SoftMajority'], ID)\n",
    "        elif row['damon'] != '-':\n",
    "            Labels[ID] = MakeOneHotLabels(row['damon'], ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSize = 24\n",
    "lstmUnits = 64\n",
    "numClasses = 5\n",
    "iterations = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "516\n",
      "499 448016\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Gettings labels and sentences together.\n",
    "# Weeding out sentences over 120 words to determine max sequence length. \n",
    "print(len(Labels))\n",
    "numWords = []\n",
    "LabelsGraph = []\n",
    "for i in Labels.keys():\n",
    "    numWords.append(len(RawSentences[i]))\n",
    "    LabelsGraph.append(Labels[i])\n",
    "\n",
    "IDs = list(Labels.keys())\n",
    "for ID in IDs:\n",
    "    if len(RawSentences[ID]) > 120:\n",
    "        del Labels[ID]\n",
    "print(len(Labels), len(RawSentences))\n",
    "IDs = list(Labels.keys())\n",
    "random.shuffle(IDs)\n",
    "\n",
    "TrainIDs = IDs[:432]\n",
    "TestIDs = IDs[432:]\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "499\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Figuring out embedding vectors for tensor graph.\n",
    "\"\"\"\n",
    "MAXSEQUENCELENGTH = 120\n",
    "numSentencesLabeled = len(Labels)\n",
    "WORDEMBEDDINGSIZE = 300\n",
    "print(numSentencesLabeled)\n",
    "\n",
    "\n",
    "Embeddings = np.zeros((len(model.wv.vocab), WORDEMBEDDINGSIZE), dtype='float32')\n",
    "\n",
    "WordsToInd = {}\n",
    "for i, word in enumerate(model.wv.vocab):\n",
    "    WordsToInd[word] = i\n",
    "    Embeddings[i] = model.wv[word]\n",
    "    \n",
    "WordIndex = np.array([i for i in WordsToInd.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create Batches\n",
    "\"\"\"\n",
    "    \n",
    "def GetWordIndices(ID):\n",
    "    Array = np.zeros([MAXSEQUENCELENGTH], dtype='int32')\n",
    "    for j, word in enumerate(RawSentences[ID]):\n",
    "        if word in WordsToInd:\n",
    "            Array[j] = WordsToInd[word]\n",
    "        else:\n",
    "            Array[j] = -1\n",
    "            print(\"Cra[]\")\n",
    "    return Array\n",
    "\n",
    "\n",
    "def GetTrainBatch():\n",
    "    TrainLabels = np.zeros([batchSize, numClasses], dtype='int32')\n",
    "    TrainSet = np.zeros([batchSize, MAXSEQUENCELENGTH], dtype='int32')\n",
    "    IDsBatched = set()\n",
    "    Count = 0\n",
    "    while Count < batchSize:\n",
    "        SelectID = TrainIDs[random.randint(0, len(TrainIDs)-1)]\n",
    "        if SelectID not in IDsBatched:\n",
    "            IDsBatched.add(SelectID)\n",
    "            TrainSet[Count] = GetWordIndices(SelectID)\n",
    "            TrainLabels[Count] = Labels[SelectID]\n",
    "            Count += 1\n",
    "    return TrainSet, TrainLabels\n",
    "\n",
    "def GetTestBatch():\n",
    "    TestLabels = np.zeros([batchSize, numClasses])\n",
    "    TestSet = np.zeros([batchSize, MAXSEQUENCELENGTH])\n",
    "    TestIDsCovered = set()\n",
    "    Count = 0\n",
    "    while Count < batchSize:\n",
    "        SelectID = TestIDs[random.randint(0, len(TestIDs)-1)]\n",
    "        \n",
    "        if SelectID not in TestIDsCovered:\n",
    "            TestIDsCovered.add(SelectID)\n",
    "            TestSet[Count] = GetWordIndices(SelectID)\n",
    "            TestLabels[Count] = Labels[SelectID]\n",
    "            Count += 1\n",
    "\n",
    "    return TestSet, TestLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "labels = tf.placeholder(tf.float32, [batchSize, numClasses])\n",
    "inputs = tf.placeholder(tf.int32, [batchSize, MAXSEQUENCELENGTH])\n",
    "\n",
    "data = tf.Variable(tf.zeros([batchSize, MAXSEQUENCELENGTH, WORDEMBEDDINGSIZE]), dtype='float32')\n",
    "data = tf.nn.embedding_lookup(Embeddings, inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Tensor.get_shape of <tf.Tensor 'rnn/transpose_1:0' shape=(24, 120, 64) dtype=float32>>\n"
     ]
    }
   ],
   "source": [
    "lstmCell = tf.nn.rnn_cell.LSTMCell(lstmUnits)\n",
    "lstmCell = tf.contrib.rnn.DropoutWrapper(cell=lstmCell, output_keep_prob=0.75)\n",
    "initialState = lstmCell.zero_state(batchSize, dtype='float32') \n",
    "value, _ = tf.nn.dynamic_rnn(cell=lstmCell, inputs=data, initial_state=initialState, dtype='float32')\n",
    "print(value.get_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = tf.Variable(tf.truncated_normal([lstmUnits, numClasses]))\n",
    "bias = tf.Variable(tf.constant(0.1, shape=[numClasses]))\n",
    "value = tf.transpose(value, [1, 0, 2])\n",
    "last = tf.gather(value, int(value.get_shape()[0])-1)\n",
    "prediction = tf.matmul(last, weight) + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "correctPred = tf.equal(tf.argmax(prediction,1), tf.argmax(labels,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correctPred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=prediction, labels=labels))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py:1662: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "tf.summary.scalar('Loss', loss)\n",
    "tf.summary.scalar('Accuracy', accuracy)\n",
    "merged = tf.summary.merge_all()\n",
    "logdir = \"tensorbiard/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "writer = tf.summary.FileWriter(logdir, sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for i in range(iterations):\n",
    "   #Next Batch of reviews\n",
    "   nextBatch, nextBatchLabels = GetTrainBatch()\n",
    "   sess.run(optimizer, {inputs: nextBatch, labels: nextBatchLabels})\n",
    "\n",
    "   #Write summary to Tensorboard\n",
    "   if i % 50 == 0:\n",
    "       summary = sess.run(merged, {inputs: nextBatch, labels: nextBatchLabels})\n",
    "       writer.add_summary(summary, i)\n",
    "\n",
    "   #Save the network every 10,000 training iterations\n",
    "   if i % 10000 == 0 and i != 0:\n",
    "       save_path = saver.save(sess, \"models/pretrained_lstm.ckpt\", global_step=i)\n",
    "       print(\"saved to %s\" % save_path)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for this batch: 58.33333134651184\n",
      "Accuracy for this batch: 41.66666567325592\n",
      "Accuracy for this batch: 41.66666567325592\n",
      "Accuracy for this batch: 37.5\n",
      "Accuracy for this batch: 41.66666567325592\n",
      "Accuracy for this batch: 45.83333432674408\n",
      "Accuracy for this batch: 45.83333432674408\n",
      "Accuracy for this batch: 37.5\n",
      "Accuracy for this batch: 66.66666865348816\n",
      "Accuracy for this batch: 58.33333134651184\n"
     ]
    }
   ],
   "source": [
    "iterations = 10\n",
    "for i in range(iterations):\n",
    "    nextBatch, nextBatchLabels = GetTestBatch();\n",
    "    print(\"Accuracy for this batch:\", (sess.run(accuracy, {inputs: nextBatch, labels: nextBatchLabels})) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_closestwords_tsnescatterplot(model, word):\n",
    "    \n",
    "    arr = np.empty((0,300), dtype='f')\n",
    "    word_labels = [word]\n",
    "\n",
    "    # get close words\n",
    "    close_words = model.similar_by_word(word, topn=30)\n",
    "    \n",
    "    # add the vector for each of the closest words to the array\n",
    "    arr = np.append(arr, np.array([model[word]]), axis=0)\n",
    "    for wrd_score in close_words:\n",
    "        wrd_vector = model[wrd_score[0]]\n",
    "        word_labels.append(wrd_score[0])\n",
    "        arr = np.append(arr, np.array([wrd_vector]), axis=0)\n",
    "        \n",
    "    # find tsne coords for 2 dimensions\n",
    "    tsne = TSNE(n_components=2, random_state=0)\n",
    "    np.set_printoptions(suppress=True)\n",
    "    Y = tsne.fit_transform(arr)\n",
    "\n",
    "    x_coords = Y[:, 0]\n",
    "    y_coords = Y[:, 1]\n",
    "    # display scatter plot\n",
    "    plt.scatter(x_coords, y_coords)\n",
    "\n",
    "    for label, x, y in zip(word_labels, x_coords, y_coords):\n",
    "        plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')\n",
    "    plt.xlim(x_coords.min()+0.00005, x_coords.max()+0.00005)\n",
    "    plt.ylim(y_coords.min()+0.00005, y_coords.max()+0.00005)\n",
    "    plt.figure(figsize=(10, 6), dpi=800)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(model['trump'])\n",
    "display_closestwords_tsnescatterplot(model, 'trump')\n",
    "#print(model.similar_by_word('trump', topn=30))\n",
    "display_closestwords_tsnescatterplot(model, 'kavanaugh')\n",
    "#model.similar_by_word('court', topn=30)\n",
    "display_closestwords_tsnescatterplot(model, 'fbi')\n",
    "#model.similar_by_word('fbi', topn=30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
